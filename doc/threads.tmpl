			+--------------------+
			|        CS 140      |
			| PROJECT 1: THREADS |
			|   DESIGN DOCUMENT  |
			+--------------------+
				   
---- GROUP ----

>> Fill in the names and email addresses of your group members.

Aaron Wilson ajw43@buffalo.edu
Aniruddh Rao anrao3@buffalo.edu

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

			     ALARM CLOCK
			     ===========

---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

struct semaphore sema_clock; : This is a 0 initialized semaphore used to block the thread when it is sleeping.

struct thread; Changed struct thread to accomodate the following variables.
struct list_elem block_elem; : This is a list_elem to store blocked threads in a blocked list.

uint8_t ticks; : Integer used to store number of ticks to wait.

void block_add(void); : Used to call semaphore down outside of the interrupt handler.

static struct list sleeping_list; : List used to store all sleeping threads, so that they can be woken up later. This is shared across threads.

struct thread blocktime { struct list_elem blocked_elem; struct thread * sleeping_thread; int tickers;} : This struct allows us to store sleeping threads in sleeping_list with an associated pointer and the ticks to wait.

static void init_thread(struct thread * t, const char * name, int priority) : Added 0 initialization for sema_clock. 

void timer_sleep(int64_t ticks) : Added non busy waiting thread sleeping functionality.

void timer_interrupt(struct intr_frame *args UNUSED) : Added handling for waking up a sleeping thread.
---- ALGORITHMS ----

>> A2: Briefly describe what happens in a call to timer_sleep(),
>> including the effects of the timer interrupt handler.

when timer_sleep() is called we put the thread into a "sleeping list" using a struct which holds the thread, and number of ticks remaining to sleep. 
Then, the function calls sema_down() on the semaphore owned by the thread, (which is initialized to 0.) through an external function in threads.c called block_add().

>> A3: What steps are taken to minimize the amount of time spent in
>> the timer interrupt handler?

We make two major changes and minimize the time in each in the following ways:

- Timer_sleep()
	We spend as little time as possible by putting it into the list only once, and calling sema_down only once.
	This avoids busy waiting and therefore wastes little time.
        Also to add on that, the sema_down too, is called out of the interrupt context!
- Timer_interrupt()
	We go through the list once regardless of the implementation of priority scheduling, 
	decrementing every ticks variable only once per iteration.
	This happens to take very little time in the interrupt, and causes no busy waiting, because we use sema_up only when the number of ticks remaining is <= 0.
	
---- SYNCHRONIZATION ----

>> A4: How are race conditions avoided when multiple threads call
>> timer_sleep() simultaneously?

Race conditions are avoided because all the threads are inserted to the shared waiting list.
We also have each thread own its own semaphore, which does not cause any kind of race condition when multiple sleep or wake.
Another way we handle this race condition is by manipulating the fact that semaphores have interrupts disabled, so this prevents the possibility of most race conditions.

>> A5: How are race conditions avoided when a timer interrupt occurs
>> during a call to timer_sleep()?

This is avoided because we use <= 0 while checking for the number of ticks, so even if we slightly undershoot we are still good.
Furthermore, we use the fact that semaphores have interrupts disable  to preventany other race conditions.

---- RATIONALE ----

>> A6: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

Another choice we had was to use a tight loop as it was before we implemented this. This choice would be unoptimal as it causes busy waiting.

By using our current design, we can avoid busy waiting. Another way we considered was not using semaphores, calling thread_block, this has worse synchronization capabilities (which has been mentioned above) and is also deemed unoptimal as per the documentation itself. This also allows our clock to be future proof, while a non semaphore implementation would possibly not be.
The third choice we considered was using the < # of ticks + current tick time > to wake up the threads, but this does not allow for priority sorting, making priority scheduling harder.
This also allows us to avoid the use of another data strucutre, at the cost of one extra operation.

Another reason why we settled on this choice because it allows for a safe, lightweight, and portable design. It is portable as it can implement both priority shceduling and MLFQS.

			 PRIORITY SCHEDULING
			 ===================

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

bool priority_comp(const struct list_elem * a, const struct list_elem * b, void * aux UNUSED); : This is a comparator used to store list of thread_blocktime elements in order of priority.

bool thread_comp(const struct list_elem * a, const struct list_elem * b, void * aux UNUSED);
: this is used to compare all threads being store in >= fashion, based on their priority.

void sema_up2(struct semaphore * sema); : Alternative semaphore without premption used for priority premption. This is because we cannot yield threads from interrupts usually, which should not happen from the timer in general, like in case of alarm.

void thread_set_priority(int priority); Used to change priority of running thread with premption.

struct list donater_list:A list of threads the thread being donated to donates to, used to implement nested donation. 

tid_t thread_create(const char * name, int priority, thread_func * function, void * aux) : Added premption and priority based addition to ready queue.

void thread_unblock(struct thread * t) : Added priority based addition to ready list.

void thread_yield(void) : Added priority based yielding to next thread, and addition of old thread to read queue.

void sema_down(struct semaphore * sema) : Added priority order based addition to waiting list of semaphore.

void sema_up(struct semaphore * sema) : Added appropriate priority based waking up of thread, and premption of woken up thread using yield.

void lock_acquire(struct lock * lock) and void lock_release(struct lock * lock) : Added priority scheduling for lock and donation functionality.

void cond_wait(struct condition * cond, struct lock * lock) : Added priority scheduling functionality to the function.

void cond_signal(struct condition * cond, struct lock * lock) : Added priority scheduling functionality to the function.

void timer_sleep(int64_t ticks) : Added priority based insertion into sleeping_list, for priority scheduling.
>> B2: Explain the data structure used to track priority donation.
>> Use ASCII art to diagram a nested donation.  (Alternately, submit a
>> .png file.)

---- ALGORITHMS ----

>> B3: How do you ensure that the highest priority thread waiting for
>> a lock, semaphore, or condition variable wakes up first?

We store these in sorted order in the semaphore/lock/condvar's list of blocked threads using a comparator that checks which element between two list elems is lesser.
When multiple threads have the highest priority, a round robin scheduling method is used, using the same comparator!

>> B4: Describe the sequence of events when a call to lock_acquire()
>> causes a priority donation.  How is nested donation handled?

When a higher priority thread attempts to aquire a lock owned by a lower priority thread, the following happens.
- The higher priority thread is added to the lock's waiting list ( in a sorted order ).
- Then, we check if the highest priority element in the list is greater than the priority of the owner of the thread.
	If it is: the priority of the thread that owns the lock is changed to the higher priority. The lower priority is saved so that it can be reverted, once the process is done running.

Nested donation is handled via a list of threads the given thread is waiting. This allows us to check which thread a particular thread is waiting for and could potentially be a donater for now. This causes the donation to one  of these threads if required, as we see, in a nested fashion.
This logic holds for at least 8 levels.

>> B5: Describe the sequence of events when lock_release() is called
>> on a lock that a higher-priority thread is waiting for.

Firstly, the lower priority thread's priority is restored to its initial priority, in case it had priority donated to it.
After that, the highest priority thread in the waiting list aquires the lock.
If the owner of the lock is the highest priority in the ready_list, it will prempt the running thread and continue running. 
Further, the old thread is removed from the donater list of all the threads waiting.

---- SYNCHRONIZATION ----

>> B6: Describe a potential race in thread_set_priority() and explain
>> how your implementation avoids it.  Can you use a lock to avoid
>> this race?
A potential race condition in thread_set_priority is when a thread that has lowered its priority, is forced to yield cpu time to a process with a higher priority, causing it to possibly lose it's resources or allowing them to be altered by the other process prempting.
This is currently not dealth with in our implementation, but can be prevented using locks, by having a thread locking the resource it is using currently.
 If it already had one and is prempted, thanks to priority donation and the way we implement set_priority, we can have a higher priority thread waiting for the resource/lock donate it's priority to the prempted thread, preventing the race condition!

Another race condition could be a case where the thread being changed, has it's priority changed at the same time due to priority donation, causing a race. This can be prevented by having this change be synchronized through a lock, quite simply in fact. This is how our implementation avoids it too.


---- RATIONALE ----

>> B7: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

To us, this is the only design we chose that was sensible. The other potential design choice was making another list specifically for priority, while basing the other threads based on time they need to sleep for.
This allows us to change the alarm clock code to accomodate priority shceduling, and makes it easy to implement semaphore priority scheduling, as we use only one. This doesn't interfere with this.

We chose an insertion sort for all our lists, as it allows us to take the highest priority thread faster, and this retrieval is likely going to be called more than inserting a thread. It also helps make premption processes faster.
This implementation also allows for a more reliable process of pre-emption. since set_priority is called only on the running thread, priority will not change during a sort, and the order will not be disrupted.

Our priority donation mechanism too, in our opinion is an efficient choice. It allows for a quick check, runtime dependent on number of locks our thread is waiting on, which make sure for reliable priority donation when nested, by using certain checks at every level. This barely takes any time and requires the least amound of change to currently functioning scheduling mechanisms, making this an obviously great choice.

			  ADVANCED SCHEDULER
			  ==================

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

static int load_avg; : Used to store the current load_avg.
int recent_cpu; : Used to store recent_cpu for current thread.
int nice; : Used to store current nice value for the given thread.

void thread_set_priority(int priority); Made it so that this function returned our current priority when mlfqs is enabled, as calculated by our scheduler.

void thread_get_priority(int priority); Made it so that this function returned our current priority when mlfqs is enabled, as calculated by our scheduler.

tid_t thread_create(const char * name, int priority, thread_func * function, void * aux) : Ignored priority when mlfqs enabled.



---- ALGORITHMS ----

>> C2: Suppose threads A, B, and C have nice values 0, 1, and 2.  Each
>> has a recent_cpu value of 0.  Fill in the table below showing the
>> scheduling decision and the priority and recent_cpu values for each
>> thread after each given number of timer ticks:

timer  recent_cpu    priority   thread
ticks   A   B   C   A   B   C   to run
-----  --  --  --  --  --  --   ------
 0      0   0   0  61  59  57       A
 4  0.27 1.00 2.00 62  60  58       A 
 8  0.54 1.16 2.33 62  60  58       A
12  0.80 1.26 2.53 62  60  58       A
16  1.06 1.35 2.71 62  60  58       A
20  1.32 1.44 2.88 62  60  58       A
24  1.85 1.52 3.05 62  60  58       A
28  1.83 1.61 3.22 62  60  58       A
32  2.07 1.69 3.38 62  60  58       A
36  2.32 1.77 3.54 62  60  58       A

>> C3: Did any ambiguities in the scheduler specification make values
>> in the table uncertain?  If so, what rule did you use to resolve
>> them?  Does this match the behavior of your scheduler?

There were no ambiguities in this limited timescale, however given more runtime there would have been ties in priority.
We would resolve these ties in our shceduler by allowing the least nice threads with tied priority to get to run first.
While calculating, we had to decide which calculations to perform first. We decided to calculate load_avg first. 
This matches our scheduler.

>> C4: How is the way you divided the cost of scheduling between code
>> inside and outside interrupt context likely to affect performance?
Most of our scheduling in our current design is being done inside the interrupt context. But, considering the way we handle our operations and fixpoint arithmetic, the cost does not cause significant effects.
We simply use the interrupt to check for the current_tick % 4 and increment each thread's recent_cpu, which can be done in the timer_interrupt function. The scheduling can be handled by the scheduler as usual, as done in priority scheduling.
The main effects, might be an increased amount for load_avg. Although this has not been seen yet, it could cause our handler to take too long, causing the thread to not actually use as much time as it would want to before the next slice, causing an unjust bloat in recent_cpu. This could be the possible effect on the performance if we continute adding load onto the handler.
---- RATIONALE ----

>> C5: Briefly critique your design, pointing out advantages and
>> disadvantages in your design choices.  If you were to have extra
>> time to work on this part of the project, how might you choose to
>> refine or improve your design?
I think most of our design choices were valid and succinct. Although we handle most of our operations within interrupt context, in my opinion the way we do it does not cause too many issues, as it may seem to. It definitely does cause a little more inaccurate load_avg or recent_cpu but not by a lot!
Our order of operations I think is perfectly fine on the other hand, since we calculate load average first (which is more efficient and accurate than doing it with recent_cpu) and then recent_cpu and priority. This allows for efficient calculation of priority and consequent prempts if any.
Our choice to use one queue to hold all priorities although causes a slight increase in time, allows for less data usage, and simpler round robin functionality.
This might though cause an increase in interrupt time at one point, as mentioned above and if we see this cause bugs often at a certain level, this may be changed along the implementation timeline. This I think is also the main disadvantage of this implementation of the MLFQS/

>> C6: The assignment explains arithmetic for fixed-point math in
>> detail, but it leaves it open to you to implement it.  Why did you
>> decide to implement it the way you did?  If you created an
>> abstraction layer for fixed-point math, that is, an abstract data
>> type and/or a set of functions or macros to manipulate fixed-point
>> numbers, why did you do so?  If not, why not?

			   SURVEY QUESTIONS
			   ================

Answering these questions is optional, but it will help us improve the
course in future quarters.  Feel free to tell us anything you
want--these questions are just to spur your thoughts.  You may also
choose to respond anonymously in the course evaluations at the end of
the quarter.

>> In your opinion, was this assignment, or any one of the three problems
>> in it, too easy or too hard?  Did it take too long or too little time?

>> Did you find that working on a particular part of the assignment gave
>> you greater insight into some aspect of OS design?

>> Is there some particular fact or hint we should give students in
>> future quarters to help them solve the problems?  Conversely, did you
>> find any of our guidance to be misleading?

>> Do you have any suggestions for the TAs to more effectively assist
>> students, either for future quarters or the remaining projects?

>> Any other comments?
